{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "by `Atwine Mugume Twinamatsiko`\n",
    "\n",
    "## What is a decision tree?\n",
    "\n",
    "A decision tree is a `flowchart-like tree structure` that is used in modeling data, where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. \n",
    "\n",
    "It learns to partition on the basis of the attribute value. \n",
    "\n",
    "It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. The decision tree is very easy to understand, because its very relatable in terms of human decision making such as below.\n",
    "\n",
    "The decision rules are generally in form of if-then-else statements. The deeper the tree, the more complex the rules and fitter the model.\n",
    "\n",
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) `that is mostly used in classification problems.`\n",
    "\n",
    "<p>\n",
    "\n",
    "<img src='d1.jpeg'/> </p>\n",
    "\n",
    "\n",
    "Example:-\n",
    "\n",
    "Let’s say we have `a sample of 30 students` with three variables `Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft).` 15 out of these 30 `play cricket` in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? \n",
    "\n",
    "In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.\n",
    "\n",
    "This is where decision tree helps, it will segregate the students based on all values of three variable and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.\n",
    "\n",
    "<img src='d2.png'/>\n",
    "\n",
    "\n",
    ">> __As mentioned above, decision tree identifies the most significant variable and it’s value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To do this, decision tree uses various algorithms, which we will discuss in the following section.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Decision Trees:-\n",
    "\n",
    "Types of decision tree is based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "**Categorical Variable Decision Tree:** Decision Tree which has categorical target variable then it called as categorical variable decision tree. Example:- In above scenario of student problem, where the target variable was `“Student will play cricket or not” i.e. YES or NO.`\n",
    "\n",
    "**Continuous Variable Decision Tree:** Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n",
    "\n",
    "## Important Terminology related to Decision Trees\n",
    "\n",
    "Let’s look at the basic terminology used with Decision trees:\n",
    "\n",
    "- **Root Node:** It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "- **Splitting:** It is a process of dividing a node into two or more sub-nodes.\n",
    "\n",
    "- **Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "\n",
    "- **Leaf/ Terminal Node:** Nodes do not split further then is called Leaf or Terminal node.\n",
    "\n",
    "- **Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "\n",
    "- **Branch / Sub-Tree:** A sub section of entire tree is called branch or sub-tree.\n",
    "\n",
    "- **Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "\n",
    "<img src='d3.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- **Easy to Understand:** Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "\n",
    "- **Useful in Data exploration:** Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables / features that has better power to predict target variable. \n",
    "\n",
    "- **Less data cleaning required:** It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.\n",
    "- **Data type is not a constraint:** It can handle both numerical and categorical variables.\n",
    "- **Non Parametric Method:** Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n",
    " \n",
    "\n",
    "### Disadvantages\n",
    "- **Over fitting:** Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "- **Not fit for continuous variables:** While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a tree decide where to split?\n",
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.\n",
    "\n",
    "Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "The algorithm selection is also based on type of target variables. Let’s look at the four most commonly used algorithms in decision tree:\n",
    "\n",
    "### Gini\n",
    "Gini  says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "\n",
    "- It works with categorical target variable “Success” or “Failure”.\n",
    "- It performs only Binary splits\n",
    "- Higher the value of Gini higher the homogeneity.\n",
    "- CART (Classification and Regression Tree) uses Gini method to create binary splits.\n",
    "\n",
    "#### Steps to Calculate Gini for a split\n",
    "\n",
    "- Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n",
    "- Calculate Gini for split using weighted Gini score of each node of that split.\n",
    "\n",
    "### Example: – \n",
    "\n",
    "Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). In the snapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini.\n",
    "\n",
    "<img src='d4.png'/>\n",
    "\n",
    "### Split on Gender:\n",
    "\n",
    "- Calculate, Gini for sub-node Female = (0.2)*(0.2)+(0.8)*(0.8)=0.68\n",
    "- Gini for sub-node Male = (0.65)*(0.65)+(0.35)*(0.35)=0.55\n",
    "- Calculate weighted Gini for Split Gender = (10/30)*0.68+(20/30)*0.55 = **0.59**\n",
    "\n",
    "### Similar for Split on Class:\n",
    "\n",
    "- Gini for sub-node Class IX = (0.43)*(0.43)+(0.57)*(0.57)=0.51\n",
    "- Gini for sub-node Class X = (0.56)*(0.56)+(0.44)*(0.44)=0.51\n",
    "- Calculate weighted Gini for Split Class = (14/30)*0.51+(16/30)*0.51 = 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain:\n",
    "\n",
    "Look at the image below and think which node can be described easily. I am sure, your answer is C because it requires less information as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say that C is a Pure node, B is less Impure and A is more impure.\n",
    "\n",
    "<img src='d5.png'/>\n",
    "\n",
    "Now, we can build a conclusion that less impure node requires less information to describe it. And, more impure node requires more information. Information theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% – 50%), it has entropy of one.\n",
    "\n",
    "Entropy can be calculated using formula:- `Entropy, Decision Tree`\n",
    "\n",
    "#### Steps to calculate entropy for a split:\n",
    "\n",
    "Calculate entropy of parent node\n",
    "Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the key parameters of tree modeling and how can we avoid over-fitting in decision trees?\n",
    "\n",
    "Overfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\n",
    "\n",
    "- Setting constraints on tree size\n",
    "- Tree pruning.\n",
    "\n",
    "Lets discuss both of these briefly.\n",
    "\n",
    "### Setting Constraints on Tree Size.\n",
    "\n",
    "This can be done by using various parameters which are used to define a tree. First, lets look at the general structure of a decision tree:\n",
    "\n",
    "<img src='d6.webp'/>\n",
    "\n",
    "The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It is important to understand the role of parameters used in tree modeling. These parameters are available in R & Python\n",
    "\n",
    "### Minimum samples for a node split\n",
    "- Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "- Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "- Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "### Minimum samples for a terminal node (leaf)\n",
    "- Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    "- Used to control over-fitting similar to min_samples_split.\n",
    "- Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n",
    "\n",
    "### Maximum depth of tree (vertical depth)\n",
    "- The maximum depth of a tree.\n",
    "- Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "- Should be tuned using CV.\n",
    "\n",
    "### Maximum number of terminal nodes\n",
    "- The maximum number of terminal nodes or leaves in a tree.\n",
    "- Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "\n",
    "### Maximum features to consider for split\n",
    "- The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "- As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "- Higher values can lead to over-fitting but depends on case to case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Pruning\n",
    "As discussed earlier, the technique of setting constraint is a greedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Let’s consider the following case when you’re driving:\n",
    "\n",
    "<img src='d7.png'/>\n",
    "\n",
    "__There are 2 lanes:__\n",
    "\n",
    "- A lane with cars moving at 80km/h\n",
    "- A lane with trucks moving at 30km/h\n",
    "\n",
    "__At this instant, you are the yellow car and you have 2 choices:__\n",
    "\n",
    "- Take a left and overtake the other 2 cars quickly\n",
    "- Keep moving in the present lane\n",
    "\n",
    "Lets analyze these choice. In the former choice, you’ll immediately overtake the car ahead and reach behind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!\n",
    "\n",
    "This is exactly the difference between normal decision tree & pruning. A decision tree with constraints won’t see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.\n",
    "\n",
    "So we know pruning is better. But how to implement it in decision tree? The idea is simple.\n",
    "\n",
    "- We first make the decision tree to a large depth.\n",
    "- Then we start at the bottom and start removing leaves which are giving us negative returns when compared from the top.\n",
    "- Suppose a split is giving us a gain of say -10 (loss of 10) and then the next split on that gives us a gain of 20. A simple decision tree will stop at step 1 but in pruning, we will see that the overall gain is +10 and keep both leaves.\n",
    "\n",
    "Note that sklearn’s decision tree classifier does not currently support pruning. Advanced packages like xgboost have adopted tree pruning in their implementation. But the library rpart in R, provides a function to prune. Good for R users!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "\n",
    "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"700\" height=\"500\" src=\"https://www.youtube.com/embed/qDcl-FRnwSU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"700\" height=\"500\" src=\"https://www.youtube.com/embed/qDcl-FRnwSU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
